Our codes are building upon existing generative advarsaril networks. Some parameters are loading from existing pre-trained models.

Thanks to the following reference:

@inproceedings{zhu2020semantically,
  title={Semantically multi-modal image synthesis},
  author={Zhu, Zhen and Xu, Zhiliang and You, Ansheng and Bai, Xiang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5467--5476},
  year={2020}
}

@inproceedings{isola2017image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

https://colab.research.google.com/github/moein-shariatnia/Deep-Learning/blob/main/Image%20Colorization%20Tutorial/Image%20Colorization%20with%20U-Net%20and%20GAN%20Tutorial.ipynb

